机器之心：在2015年在ImageNet测试中，您带领团队使用了152层神经网络，取得了三个主要项目的冠军。您和您的团队是如何想到这个方法，又是怎样去实现的？孙剑：很多时候做研究，是在无数次的尝试中最后总结出的方法，同时把一个复杂的方法进行简化。做这个（残差网络），我们试了非常多的方法，有一些方法我们都没有公布。中间经历过很多，做了实验之后最后总结出（残差网络），发现它非常有效。找到这个有效的方式后，我们分析它的原理，为什么能起作用。最后在论文中以残差学习的形式呈现，这是我们当时认为最好的一种解释。后来很多人尝试新的解释和改进，也有A解释、B解释、C解释，有些我们是认同的，有些我们不认同，其实蛮有意思的。残差网络并不是说做到多少层，而是你也可以简单的做到这么多层，它核心使深层网络的优化变得容易。残差网络相当于将问题重新描述，但本质没变，以至于用现有的优化算法就很好解。以前不收敛，现在就能收敛了；以前收敛到很差的结果，现在就非常容易收敛到很好结果，所以它本质上是解决了优化问题。这个问题困扰了神经网络工作者非常长时间。为什么叫深度学习？深度就是网络层数，层数越多就越深，刚开始做5层就算深度。2012年GeoffreyHinton做了8层，他的论文专门写了一段证明8层比5层好，越深越好，因为还有很多人不相信这是有道理的。就算他们做得已经很好了，还有一些论文中说浅的网络也能做得一样好，「深」是不必要的。在神经网络研究的历史中，很长时间内大家不相信那么深的网络能够优化出来。做深度学习之前大家研究SVM（SupportVectorMachine，支持向量机），研究稀疏表示，很大程度上是线性问题。大家试图研究凸的（问题），非凸的还想办法转成凸的做，对于这么深的网络、这么复杂的事情、高度非线性又有这么多参数，数据又很少，很多人都不相信能把它优化出来。今天能够相当程度地解决也包含很多因素。残差学习是其中一个重要因素，但不是唯一的。
把大家研究出来的结论放在一起，才导致今天任意给一个深度网络都能很容易地训练出来，深度再也不是网络不收敛训练不好的问题，破除了以前的魔咒。最后要说一下做出这个残差网络完全是团队（何凯明、张祥雨、任少卿和我）的集体智慧结晶，缺少任何一人都不敢说能走得到这一步，中间经历很多的失败和曲折。我深感能把我们4个不同技能的人凑在一起，打下一个「大怪兽」的幸运；和他们在一起忘我的研究过程是我研究生涯中最难忘的经历之一。机器之心：在图像识别之外，残差网络还可以运用到其它领域吗？孙剑：最近语音识别、自然语言处理都在用。它是一种思想，并不是一个局限于图像识别的一个方法。这个思想用在别的地方都管用，我们看到了非常多的例子，大公司、小公司都在用。而且最先进的系统、最复杂的系统都在用这个思想。并不是简单的用残差网络这个方式做，比如语言处理中的一个环节想要做做深，原来两层就不行，现在可以做得很深。用残差学习或跳层连接做得很深，效果很好，训练也很容易。并不是说以前不能搭这么深，搭这么深结果更差，现在有自由度想搭多深搭多深。当然也不是说越深越好，跟问题和数据都有关系。考虑复杂度和效果肯定是找折衷点，不过现在不受深度的约束了。机器之心：那您还会继续残差网络的研究吗？孙剑：这是我们的一个中间结果。我觉得残差网络是一方面，但是我们做研究希望找下一个大想法，当然结构可能融合了残差网络方法，因为它这个很好的思想并不是具体的一个网络。后来有很多人开发各种网络，结构都不一样，但残差网络的思想是其中必不可少的部分。现在所有网络都是残差网络，重点已经不是加残差网络了，而是说在以加了它为基础的情况下，再去研究别的特性，把这个问题再深刻理解，怎么能够做得更好。举个例子，分类能做得好，但这个网络未必适合于检测、分割这样的问题。只有把问题理解更深入，才能设计出最适合特定问题的网络。

